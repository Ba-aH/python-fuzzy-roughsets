{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vehicle Dataset Classification with Fuzzy Rough Sets\n",
        "\n",
        "This notebook demonstrates how to use the Eddy library (Fuzzy LEM2) for vehicle classification.\n",
        "\n",
        "## Dataset Information\n",
        "- **846 samples** of vehicle silhouettes\n",
        "- **18 continuous features** (shape measurements)\n",
        "- **4 classes**: van, saab, bus, opel\n",
        "\n",
        "## What is Fuzzy LEM2?\n",
        "LEM2 (Learning from Examples Module 2) is a rule induction algorithm based on rough set theory. The fuzzy extension handles continuous data by using fuzzy membership degrees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    accuracy_score,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# Eddy library imports\n",
        "from eddy.fuzzylem import FuzzyLEM2Classifier\n",
        "import eddy.datasets as data\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: vehicle\n",
            "Shape: (768, 8)\n",
            "Samples: 768\n",
            "Features: 8\n",
            "Classes: 1\n"
          ]
        }
      ],
      "source": [
        "# Load the vehicle dataset\n",
        "(X, y), ds_name = data.vehicle()\n",
        "\n",
        "print(f\"Dataset: {ds_name}\")\n",
        "print(f\"Shape: {X.shape}\")\n",
        "print(f\"Samples: {X.shape[0]}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {len(np.unique(y))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature names (from the CSV header)\n",
        "feature_names = [\n",
        "    'Compactness', 'Circularity', 'Distance_circularity', 'Radius_ratio',\n",
        "    'Praxis_aspect_ratio', 'Max_length_aspect_ratio', 'Scatter_ratio',\n",
        "    'Elongatedness', 'Praxis_rectangular', 'Length_rectangular',\n",
        "    'Major_variance', 'Minor_variance', 'Gyration_radius',\n",
        "    'Major_skewness', 'Minor_skewness', 'Minor_kurtosis',\n",
        "    'Major_kurtosis', 'Hollows_ratio'\n",
        "]\n",
        "\n",
        "class_names = ['van', 'saab', 'bus', 'opel']\n",
        "\n",
        "# Create a DataFrame for easier exploration\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['Class'] = y\n",
        "df['Class_Name'] = df['Class'].map({0: 'van', 1: 'saab', 2: 'bus', 3: 'opel'})\n",
        "\n",
        "print(\"\\nüìä First few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Exploration and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "class_counts = df['Class_Name'].value_counts()\n",
        "axes[0].bar(class_counts.index, class_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'])\n",
        "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Vehicle Type')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count labels on bars\n",
        "for i, (name, count) in enumerate(class_counts.items()):\n",
        "    axes[0].text(i, count + 5, str(count), ha='center', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',\n",
        "            colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'], startangle=90)\n",
        "axes[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìà Class Statistics:\")\n",
        "for name in class_names:\n",
        "    count = (df['Class_Name'] == name).sum()\n",
        "    percentage = count / len(df) * 100\n",
        "    print(f\"   {name:>6s}: {count:3d} samples ({percentage:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature statistics\n",
        "print(\"\\nüìê Feature Statistics:\")\n",
        "df[feature_names].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some key features by class\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "features_to_plot = ['Compactness', 'Circularity', 'Elongatedness', \n",
        "                    'Scatter_ratio', 'Major_variance', 'Minor_variance']\n",
        "\n",
        "for idx, feature in enumerate(features_to_plot):\n",
        "    for class_name in class_names:\n",
        "        data_subset = df[df['Class_Name'] == class_name][feature]\n",
        "        axes[idx].hist(data_subset, alpha=0.5, label=class_name, bins=20)\n",
        "    \n",
        "    axes[idx].set_title(f'{feature} Distribution', fontweight='bold')\n",
        "    axes[idx].set_xlabel(feature)\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap (first 10 features for readability)\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = df[feature_names[:10]].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            square=True, cbar_kws={'label': 'Correlation'})\n",
        "plt.title('Feature Correlation Heatmap (First 10 Features)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining class distribution:\")\n",
        "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
        "for class_idx, count in train_dist.items():\n",
        "    print(f\"   {class_names[int(class_idx)]}: {count} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Fuzzy LEM2 Classifier\n",
        "\n",
        "### Parameters Explanation:\n",
        "- **alpha** (0.05): Dependency threshold - controls how strict the rules are\n",
        "  - Lower values ‚Üí stricter rules (fewer false positives)\n",
        "  - Higher values ‚Üí more lenient rules (better coverage)\n",
        "  \n",
        "- **beta** (0.2): Covering threshold - controls how precisely rules must cover the concept\n",
        "  - Lower values ‚Üí more precise covering required\n",
        "  - Higher values ‚Üí allows partial covering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the classifier\n",
        "clf = FuzzyLEM2Classifier(alpha=0.05, beta=0.2)\n",
        "\n",
        "print(\"ü§ñ Training Fuzzy LEM2 Classifier...\")\n",
        "print(\"‚è≥ This may take a few minutes...\\n\")\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the generated rules\n",
        "print(\"\\nüìã Generated Rules Summary:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "total_rules = 0\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    rules = clf.rules_[class_idx]\n",
        "    num_rules = len(rules)\n",
        "    total_rules += num_rules\n",
        "    print(f\"{class_name:>6s}: {num_rules:3d} rule complexes\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total:  {total_rules:3d} rule complexes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Make Predictions and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "print(\"üîÆ Making predictions on test set...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nüéØ Overall Accuracy: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìà CLASSIFICATION REPORT\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "print(classification_report(y_test, y_pred, target_names=class_names, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Confusion matrix - counts\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp1.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Confusion matrix - percentages\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=class_names)\n",
        "disp2.plot(ax=axes[1], cmap='Greens', values_format='.1f')\n",
        "axes[1].set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class performance\n",
        "print(\"\\nüìä Per-Class Performance:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "per_class_data = []\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    tp = cm[i, i]\n",
        "    total = cm[i].sum()\n",
        "    accuracy_class = tp / total if total > 0 else 0\n",
        "    \n",
        "    # Calculate precision and recall\n",
        "    precision = tp / cm[:, i].sum() if cm[:, i].sum() > 0 else 0\n",
        "    recall = accuracy_class\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    per_class_data.append({\n",
        "        'Class': name,\n",
        "        'Accuracy': accuracy_class,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    })\n",
        "    \n",
        "    print(f\"{name:>6s}: Acc={accuracy_class:.2%}, Prec={precision:.2%}, Rec={recall:.2%}, F1={f1:.3f}\")\n",
        "\n",
        "# Visualize per-class performance\n",
        "perf_df = pd.DataFrame(per_class_data)\n",
        "perf_df.set_index('Class')[['Accuracy', 'Precision', 'Recall', 'F1-Score']].plot(\n",
        "    kind='bar', figsize=(12, 6), rot=0\n",
        ")\n",
        "plt.title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Example Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some example predictions\n",
        "print(\"\\nüîç Example Predictions (First 10 test samples):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Sample':>6} | {'True':>6} | {'Predicted':>10} | {'Correct':>7}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i in range(min(10, len(y_test))):\n",
        "    true_class = class_names[int(y_test[i])]\n",
        "    pred_class = class_names[int(y_pred[i])]\n",
        "    correct = \"‚úì\" if y_test[i] == y_pred[i] else \"‚úó\"\n",
        "    print(f\"{i+1:>6} | {true_class:>6} | {pred_class:>10} | {correct:>7}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Parameter Tuning Experiment (Optional)\n",
        "\n",
        "**Warning**: This section will take a long time to run! Feel free to skip it or reduce the parameter ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different parameter combinations\n",
        "alpha_values = [0.01, 0.05, 0.1]\n",
        "beta_values = [0.1, 0.2, 0.3]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"üî¨ Testing different parameter combinations...\\n\")\n",
        "print(\"This will take several minutes...\\n\")\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    for beta in beta_values:\n",
        "        print(f\"Testing alpha={alpha}, beta={beta}...\", end=\" \")\n",
        "        \n",
        "        try:\n",
        "            clf_test = FuzzyLEM2Classifier(alpha=alpha, beta=beta)\n",
        "            clf_test.fit(X_train, y_train)\n",
        "            y_pred_test = clf_test.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred_test)\n",
        "            \n",
        "            results.append({\n",
        "                'alpha': alpha,\n",
        "                'beta': beta,\n",
        "                'accuracy': acc\n",
        "            })\n",
        "            \n",
        "            print(f\"Accuracy: {acc:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Parameter tuning complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize parameter tuning results\n",
        "if results:\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Create pivot table for heatmap\n",
        "    pivot_table = results_df.pivot(index='alpha', columns='beta', values='accuracy')\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', \n",
        "                cbar_kws={'label': 'Accuracy'})\n",
        "    plt.title('Parameter Tuning Results (Alpha vs Beta)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Beta (covering threshold)')\n",
        "    plt.ylabel('Alpha (dependency threshold)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find best parameters\n",
        "    best_result = results_df.loc[results_df['accuracy'].idxmax()]\n",
        "    print(f\"\\nüèÜ Best Parameters:\")\n",
        "    print(f\"   Alpha: {best_result['alpha']}\")\n",
        "    print(f\"   Beta: {best_result['beta']}\")\n",
        "    print(f\"   Accuracy: {best_result['accuracy']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Comparison with Other Classifiers (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train different classifiers\n",
        "classifiers = {\n",
        "    'Fuzzy LEM2': clf,\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "print(\"üî¨ Comparing with other classifiers...\\n\")\n",
        "\n",
        "for name, classifier in classifiers.items():\n",
        "    if name != 'Fuzzy LEM2':\n",
        "        print(f\"Training {name}...\", end=\" \")\n",
        "        classifier.fit(X_train, y_train)\n",
        "        print(\"Done.\")\n",
        "    \n",
        "    y_pred_comp = classifier.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred_comp)\n",
        "    \n",
        "    comparison_results.append({\n",
        "        'Classifier': name,\n",
        "        'Accuracy': acc\n",
        "    })\n",
        "\n",
        "print(\"\\n‚úÖ Comparison complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "comp_df = pd.DataFrame(comparison_results).sort_values('Accuracy', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = ['#FF6B6B' if x == 'Fuzzy LEM2' else '#95E1D3' for x in comp_df['Classifier']]\n",
        "bars = plt.barh(comp_df['Classifier'], comp_df['Accuracy'], color=colors)\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, acc) in enumerate(zip(bars, comp_df['Accuracy'])):\n",
        "    plt.text(acc + 0.01, i, f'{acc:.2%}', va='center', fontweight='bold')\n",
        "\n",
        "plt.xlabel('Accuracy', fontweight='bold')\n",
        "plt.title('Classifier Comparison on Vehicle Dataset', fontsize=14, fontweight='bold')\n",
        "plt.xlim(0, 1)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Classifier Rankings:\")\n",
        "print(comp_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üìù SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n‚úÖ Successfully trained Fuzzy LEM2 classifier on vehicle dataset\")\n",
        "print(f\"\\nüìä Key Results:\")\n",
        "print(f\"   ‚Ä¢ Dataset: {X.shape[0]} samples, {X.shape[1]} features, {len(class_names)} classes\")\n",
        "print(f\"   ‚Ä¢ Test Accuracy: {accuracy:.2%}\")\n",
        "print(f\"   ‚Ä¢ Total Rules Generated: {sum(len(clf.rules_[i]) for i in range(len(class_names)))}\")\n",
        "print(f\"\\nüí° Key Observations:\")\n",
        "print(f\"   ‚Ä¢ Vehicle classification is challenging due to overlapping features\")\n",
        "print(f\"   ‚Ä¢ Fuzzy LEM2 generates interpretable if-then rules\")\n",
        "print(f\"   ‚Ä¢ Parameters (alpha, beta) significantly affect performance\")\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
